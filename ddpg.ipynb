{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import copy\n",
    "\n",
    "from dm_control import suite\n",
    "device = \"cuda\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import skimage\n",
    "from io import BytesIO\n",
    "import imageio\n",
    "from IPython.display import Image\n",
    "\n",
    "def display_video(frames, framerate=30):\n",
    "    gif_file = BytesIO()\n",
    "    imageio.mimsave(gif_file, [skimage.img_as_ubyte(frame) for frame in frames], 'GIF', fps=30)\n",
    "    return Image(data=gif_file.getvalue())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def partial_copy_model(model1,model2,t):\n",
    "    new_state_dict = model1.state_dict()\n",
    "    update_state_dict = model2.state_dict()\n",
    "\n",
    "    for key in new_state_dict.keys():\n",
    "        new_state_dict[key] = (1-t)*new_state_dict[key] + t*update_state_dict[key]\n",
    "\n",
    "    model1.load_state_dict(new_state_dict)\n",
    "\n",
    "class DDPG:\n",
    "    def __init__(self, state_size, action_size, decay=0.99, exploration_noise=0.2, target_lr=0.001):\n",
    "        self.exploration_noise = exploration_noise\n",
    "        self.target_lr = target_lr\n",
    "        self.decay = decay\n",
    "        self.target_lr = target_lr\n",
    "        self.action_size = action_size\n",
    "        self.state_size = state_size\n",
    "\n",
    "        self.critic = torch.nn.Sequential(\n",
    "            torch.nn.Linear(state_size+action_size, 200),\n",
    "            torch.nn.Tanh(),\n",
    "            torch.nn.Linear(200,200),\n",
    "            torch.nn.Tanh(),\n",
    "            torch.nn.Linear(200, 1),\n",
    "        ).to(device)\n",
    "\n",
    "        self.actor = torch.nn.Sequential(\n",
    "            torch.nn.Linear(state_size, 200),\n",
    "            torch.nn.ReLU(),\n",
    "            torch.nn.Linear(200,200),\n",
    "            torch.nn.ReLU(),\n",
    "            torch.nn.Linear(200, action_size),\n",
    "            torch.nn.Tanh(),\n",
    "        ).to(device)\n",
    "        \n",
    "        actor_state_dict = self.actor.state_dict()\n",
    "        actor_state_dict[\"4.bias\"][:] = 0\n",
    "        actor_state_dict[\"4.weight\"] = torch.normal(\n",
    "            torch.zeros_like(actor_state_dict[\"4.weight\"]),\n",
    "            0.001*torch.ones_like(actor_state_dict[\"4.weight\"]),\n",
    "        )\n",
    "        self.actor.load_state_dict(actor_state_dict)\n",
    "\n",
    "        self.target_actor = copy.deepcopy(self.actor)\n",
    "        self.target_critic = copy.deepcopy(self.critic)\n",
    "\n",
    "        self.actor.to(device)\n",
    "        self.critic.to(device)\n",
    "        self.target_actor.to(device)\n",
    "        self.target_critic.to(device)\n",
    "\n",
    "        self.critic_optimizer = torch.optim.Adam(self.critic.parameters(),lr=0.003,weight_decay=0.01)\n",
    "        self.actor_optimizer = torch.optim.Adam(self.actor.parameters(),lr=0.0003)\n",
    "\n",
    "        self.random_state = np.random.RandomState(42)\n",
    "\n",
    "        self.replay_size = 1000000\n",
    "        self.replay_ind = 0\n",
    "        self.replay_prev_state = torch.zeros((self.replay_size,state_size)).to(device)\n",
    "        self.replay_new_state = torch.zeros((self.replay_size,state_size)).to(device)\n",
    "        self.replay_action = torch.zeros((self.replay_size,action_size)).to(device)\n",
    "        self.replay_reward = torch.zeros((self.replay_size)).to(device)\n",
    "\n",
    "\n",
    "    def get_action(self,state):\n",
    "        action = self.actor(state.to(device)).cpu() + torch.normal(torch.zeros(self.action_size), std=self.exploration_noise)\n",
    "        #action = torch.normal(torch.zeros(self.action_size), std=self.exploration_noise)\n",
    "        action += (action<-1).float()\n",
    "        action -= (action>1).float()\n",
    "        return action.detach().cpu().numpy()\n",
    "\n",
    "    def store_transition(self, prev_state,new_state, action, reward, should_print=False):\n",
    "        action = torch.tensor(action).to(device)\n",
    "        # insert transition into buffer\n",
    "        self.replay_prev_state[self.replay_ind % self.replay_size] = prev_state\n",
    "        self.replay_new_state[self.replay_ind % self.replay_size] = new_state\n",
    "        self.replay_action[self.replay_ind % self.replay_size] = action\n",
    "        self.replay_reward[self.replay_ind % self.replay_size] = reward\n",
    "        self.replay_ind += 1\n",
    "\n",
    "        if should_print:\n",
    "            print(self.critic(torch.cat([prev_state,action])))\n",
    "\n",
    "        if self.random_state.uniform(0,1)>0.2:\n",
    "            return\n",
    "            \n",
    "        # sample minibatch from replay buffer\n",
    "        num_samples = min(200,self.replay_ind)\n",
    "        sample_inds = self.random_state.randint(0,min(self.replay_ind,self.replay_size), num_samples)\n",
    "        sample_prev_state = self.replay_prev_state[sample_inds].to(device)\n",
    "        sample_new_state = self.replay_new_state[sample_inds].to(device)\n",
    "        sample_action = self.replay_action[sample_inds].to(device)\n",
    "        sample_reward = self.replay_reward[sample_inds].to(device)\n",
    "\n",
    "        # calculate target reward using bellman equation with target models\n",
    "        predicted_next_reward = self.target_critic(torch.cat([sample_new_state, self.target_actor(sample_new_state)],dim=1))[:,0]\n",
    "        target_reward = sample_reward+self.decay*predicted_next_reward\n",
    "\n",
    "        # Train critic\n",
    "        self.critic_optimizer.zero_grad()\n",
    "        predicted_reward = self.critic(torch.cat([sample_prev_state,sample_action],dim=1))[:,0]\n",
    "        loss = torch.mean((target_reward - predicted_reward)**2)\n",
    "        loss.backward()\n",
    "        self.critic_optimizer.step()\n",
    "        \n",
    "\n",
    "        # Train actor\n",
    "        self.actor_optimizer.zero_grad()\n",
    "        predicted_reward = self.critic(torch.cat([sample_prev_state,self.actor(sample_prev_state)],dim=1))[:,0]\n",
    "        loss = (1-predicted_reward).mean()\n",
    "        loss.backward()\n",
    "        self.actor_optimizer.step()\n",
    "\n",
    "        # Update target networks\n",
    "        partial_copy_model(self.target_actor,self.actor,self.target_lr)\n",
    "        partial_copy_model(self.target_critic,self.critic,self.target_lr)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def flatten_obs(observation):\n",
    "    return torch.tensor(np.concatenate(list(observation.values()))).float().to(device)\n",
    "\n",
    "def simulate_render(env,agent, duration=3):\n",
    "    frames = []\n",
    "    rewards = []\n",
    "\n",
    "    spec = env.action_spec()\n",
    "    time_step = env.reset()\n",
    "    print(spec)\n",
    "    while env.physics.data.time < duration:\n",
    "        action = agent.get_action(flatten_obs(time_step.observation))\n",
    "        time_step = env.step(action)\n",
    "\n",
    "        camera0 = env.physics.render(camera_id=0, height=200, width=200)\n",
    "        camera1 = env.physics.render(camera_id=1, height=200, width=200)\n",
    "        frames.append(np.hstack((camera0, camera1)))\n",
    "        rewards.append(time_step.reward)\n",
    "    print(\"Num frames:\",len(frames))\n",
    "    return display_video(frames, framerate=1./env.control_timestep()*5)\n",
    "\n",
    "def simulate_train(env,agent, duration=3):\n",
    "    frames = []\n",
    "    rewards = []\n",
    "\n",
    "    spec = env.action_spec()\n",
    "    time_step = env.reset()\n",
    "    should_print = True\n",
    "    while env.physics.data.time < duration:\n",
    "        prev_state = flatten_obs(time_step.observation)\n",
    "        action = agent.target_actor(prev_state.to(device)).cpu().detach()\n",
    "        time_step = env.step(action)\n",
    "        agent.store_transition(prev_state, flatten_obs(time_step.observation), action, time_step.reward,should_print=should_print)\n",
    "        should_print=False\n",
    "        rewards.append(time_step.reward)\n",
    "\n",
    "    return time_step.reward"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = suite.load('cartpole', 'swingup')\n",
    "state_size = len(flatten_obs(env.reset().observation))\n",
    "action_size = env.action_spec().shape[0]\n",
    "agent = DDPG(state_size, action_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\fredr\\AppData\\Local\\Temp/ipykernel_21772/3467891947.py:73: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  action = torch.tensor(action).to(device)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([0.1698], device='cuda:0', grad_fn=<AddBackward0>)\n",
      "7.280726827154927e-05\n",
      "tensor([0.1712], device='cuda:0', grad_fn=<AddBackward0>)\n",
      "0.013963357755899112\n",
      "tensor([0.2081], device='cuda:0', grad_fn=<AddBackward0>)\n",
      "0.003001315127822956\n",
      "tensor([0.2381], device='cuda:0', grad_fn=<AddBackward0>)\n",
      "0.042120442479328585\n",
      "tensor([0.2699], device='cuda:0', grad_fn=<AddBackward0>)\n",
      "0.004878742972049342\n",
      "tensor([0.2977], device='cuda:0', grad_fn=<AddBackward0>)\n",
      "0.07053634325607723\n",
      "tensor([0.3240], device='cuda:0', grad_fn=<AddBackward0>)\n",
      "0.15214289652297341\n",
      "tensor([0.3439], device='cuda:0', grad_fn=<AddBackward0>)\n",
      "0.2488411073650475\n",
      "tensor([0.3639], device='cuda:0', grad_fn=<AddBackward0>)\n",
      "0.03577074975211909\n",
      "tensor([0.3748], device='cuda:0', grad_fn=<AddBackward0>)\n",
      "0.08371723217122462\n",
      "tensor([0.3932], device='cuda:0', grad_fn=<AddBackward0>)\n",
      "0.07096625053740134\n",
      "tensor([0.4065], device='cuda:0', grad_fn=<AddBackward0>)\n",
      "0.05226525562814396\n",
      "tensor([0.4149], device='cuda:0', grad_fn=<AddBackward0>)\n",
      "0.045876825177680235\n",
      "tensor([0.4236], device='cuda:0', grad_fn=<AddBackward0>)\n",
      "0.05814376397390234\n",
      "tensor([0.4372], device='cuda:0', grad_fn=<AddBackward0>)\n",
      "0.05352381571235045\n",
      "tensor([0.4387], device='cuda:0', grad_fn=<AddBackward0>)\n",
      "0.03958170346528984\n",
      "tensor([0.4468], device='cuda:0', grad_fn=<AddBackward0>)\n",
      "0.037188278941477984\n",
      "tensor([0.4552], device='cuda:0', grad_fn=<AddBackward0>)\n",
      "0.02695084183213431\n",
      "tensor([0.4627], device='cuda:0', grad_fn=<AddBackward0>)\n",
      "0.0504828086487597\n",
      "tensor([0.4648], device='cuda:0', grad_fn=<AddBackward0>)\n",
      "0.041875023379585\n",
      "tensor([0.4658], device='cuda:0', grad_fn=<AddBackward0>)\n",
      "0.037364239935719974\n",
      "tensor([0.4793], device='cuda:0', grad_fn=<AddBackward0>)\n",
      "0.06248744900636205\n",
      "tensor([0.4858], device='cuda:0', grad_fn=<AddBackward0>)\n",
      "0.04543076597825733\n",
      "tensor([0.4810], device='cuda:0', grad_fn=<AddBackward0>)\n",
      "0.029621259927078637\n",
      "tensor([0.4916], device='cuda:0', grad_fn=<AddBackward0>)\n",
      "0.04921025580840455\n",
      "tensor([0.4905], device='cuda:0', grad_fn=<AddBackward0>)\n",
      "0.05518559943018535\n",
      "tensor([0.4938], device='cuda:0', grad_fn=<AddBackward0>)\n",
      "0.03928376884925964\n",
      "tensor([0.4962], device='cuda:0', grad_fn=<AddBackward0>)\n",
      "0.03251544827618438\n",
      "tensor([0.5069], device='cuda:0', grad_fn=<AddBackward0>)\n",
      "0.053497345221393544\n",
      "tensor([0.5067], device='cuda:0', grad_fn=<AddBackward0>)\n",
      "0.047079910972644856\n",
      "tensor([0.5124], device='cuda:0', grad_fn=<AddBackward0>)\n",
      "0.04980805583051019\n",
      "tensor([0.5141], device='cuda:0', grad_fn=<AddBackward0>)\n",
      "0.042410966290232964\n",
      "tensor([0.5084], device='cuda:0', grad_fn=<AddBackward0>)\n",
      "0.04560929523049735\n",
      "tensor([0.5154], device='cuda:0', grad_fn=<AddBackward0>)\n",
      "0.05257635840606862\n",
      "tensor([0.5116], device='cuda:0', grad_fn=<AddBackward0>)\n",
      "0.049865259481499724\n",
      "tensor([0.5209], device='cuda:0', grad_fn=<AddBackward0>)\n",
      "0.05601340645366796\n",
      "tensor([0.5282], device='cuda:0', grad_fn=<AddBackward0>)\n",
      "0.04571682795111936\n",
      "tensor([0.5214], device='cuda:0', grad_fn=<AddBackward0>)\n",
      "0.04954843471598785\n",
      "tensor([0.5175], device='cuda:0', grad_fn=<AddBackward0>)\n",
      "0.051707471166854334\n",
      "tensor([0.5289], device='cuda:0', grad_fn=<AddBackward0>)\n",
      "0.04306829805319401\n",
      "tensor([0.5260], device='cuda:0', grad_fn=<AddBackward0>)\n",
      "0.036191828708567575\n",
      "tensor([0.5274], device='cuda:0', grad_fn=<AddBackward0>)\n",
      "0.04218751221497086\n",
      "tensor([0.5299], device='cuda:0', grad_fn=<AddBackward0>)\n",
      "0.03770315700345213\n",
      "tensor([0.5397], device='cuda:0', grad_fn=<AddBackward0>)\n",
      "0.03935508060184446\n",
      "tensor([0.5412], device='cuda:0', grad_fn=<AddBackward0>)\n",
      "0.04518138969553567\n",
      "tensor([0.5378], device='cuda:0', grad_fn=<AddBackward0>)\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_21772/323546317.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m1000000\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m     \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msimulate_train\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0menv\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0magent\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mduration\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m5\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      3\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mi\u001b[0m\u001b[1;33m%\u001b[0m\u001b[1;36m100\u001b[0m\u001b[1;33m==\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m         \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msave\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0magent\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcritic\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;34m\"critic\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_21772/2526144920.py\u001b[0m in \u001b[0;36msimulate_train\u001b[1;34m(env, agent, duration)\u001b[0m\n\u001b[0;32m     29\u001b[0m     \u001b[1;32mwhile\u001b[0m \u001b[0menv\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mphysics\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtime\u001b[0m \u001b[1;33m<\u001b[0m \u001b[0mduration\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     30\u001b[0m         \u001b[0mprev_state\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mflatten_obs\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtime_step\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mobservation\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 31\u001b[1;33m         \u001b[0maction\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0magent\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtarget_actor\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mprev_state\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mto\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcpu\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdetach\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     32\u001b[0m         \u001b[0mtime_step\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0menv\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0maction\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     33\u001b[0m         \u001b[0magent\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstore_transition\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mprev_state\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mflatten_obs\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtime_step\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mobservation\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0maction\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtime_step\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mreward\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mshould_print\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mshould_print\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\fredr\\repos\\rl_test\\.venv\\lib\\site-packages\\torch\\nn\\modules\\module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m   1049\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[0;32m   1050\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[1;32m-> 1051\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1052\u001b[0m         \u001b[1;31m# Do not call functions when jit is used\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1053\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\fredr\\repos\\rl_test\\.venv\\lib\\site-packages\\torch\\nn\\modules\\container.py\u001b[0m in \u001b[0;36mforward\u001b[1;34m(self, input)\u001b[0m\n\u001b[0;32m    137\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minput\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    138\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mmodule\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 139\u001b[1;33m             \u001b[0minput\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmodule\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    140\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0minput\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    141\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\fredr\\repos\\rl_test\\.venv\\lib\\site-packages\\torch\\nn\\modules\\module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m   1049\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[0;32m   1050\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[1;32m-> 1051\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1052\u001b[0m         \u001b[1;31m# Do not call functions when jit is used\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1053\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\fredr\\repos\\rl_test\\.venv\\lib\\site-packages\\torch\\nn\\modules\\activation.py\u001b[0m in \u001b[0;36mforward\u001b[1;34m(self, input)\u001b[0m\n\u001b[0;32m    100\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    101\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minput\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mTensor\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m->\u001b[0m \u001b[0mTensor\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 102\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mF\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrelu\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minplace\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0minplace\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    103\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    104\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mextra_repr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m->\u001b[0m \u001b[0mstr\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\fredr\\repos\\rl_test\\.venv\\lib\\site-packages\\torch\\nn\\functional.py\u001b[0m in \u001b[0;36mrelu\u001b[1;34m(input, inplace)\u001b[0m\n\u001b[0;32m   1296\u001b[0m         \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrelu_\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1297\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1298\u001b[1;33m         \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrelu\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1299\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[0mresult\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1300\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "for i in range(1000000):\n",
    "    print(simulate_train(env,agent,duration=5))\n",
    "\n",
    "    if i%100==0:\n",
    "        torch.save(agent.critic,\"critic\")\n",
    "        torch.save(agent.actor,\"actor\")\n",
    "        torch.save(agent.target_critic,\"target_critic\")\n",
    "        torch.save(agent.target_actor,\"target_actor\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "display(simulate_render(env,agent, duration=9))"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "759a1ba3f66eddc6e5f3accc4a22b3aaea0d2c1d222398eb3317d1319880056b"
  },
  "kernelspec": {
   "display_name": "Python 3.9.1 64-bit ('.venv': venv)",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.1"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
